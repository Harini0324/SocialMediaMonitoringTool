{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZvejtnQ68mKIs/QTZCXSf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob\n",
        "!pip install schedule\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0In8N9t2EDXw",
        "outputId": "2d06fbb6-c2e7-40ee-fe2d-557a8e506156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting schedule\n",
            "  Downloading schedule-1.2.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: schedule\n",
            "Successfully installed schedule-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QvSNoeCDMcS",
        "outputId": "7618e180-bc08-4e34-885f-52639f8b8a22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis completed. Analyzed dataset saved to analyzed_datacp.csv.\n",
            "Identification of potential security threats completed. Flagged posts saved to flagged_posts.csv.\n",
            "Alert generation completed. Alerts saved to alerts.csv.\n",
            "Hello jmbirney, we have detected a potential security threat in your recent post: 'I reaaly miss john mayer's twitters.  fuck rude people. Remove him if you don't like his tweets, bitchfucks.'. Please review our terms of service and ensure compliance.\n",
            "Hello RooookieP, we have detected a potential security threat in your recent post: '@Artistbabee but tht's annoying &amp;definitely not bitchy enough  he def broke like everything in me. he prolly doesn't even kno oct. 5!'. Please review our terms of service and ensure compliance.\n",
            "Hello xtriKARAtopsx, we have detected a potential security threat in your recent post: '@kristinabitch I can't believe she's going to be gone in less than 24 hours. I'm going to miss her. '. Please review our terms of service and ensure compliance.\n",
            "Hello dirtyrottengoss, we have detected a potential security threat in your recent post: '@Nathaniel_3 well im someone's bitch. I cant say no cause i feel guilty. Even though i hate it. And my arms are sore from moving fixtures '. Please review our terms of service and ensure compliance.\n",
            "Hello seregon, we have detected a potential security threat in your recent post: '@Yourmumsmabitch Well I was there for quite a bit when your car stopped working :p Damn I forgot about your car sorry  Poor thing'. Please review our terms of service and ensure compliance.\n",
            "Hello agouty, we have detected a potential security threat in your recent post: '@tweetabitch Coming through Greensboro just a minute ago - sorry we didn't bring any fish '. Please review our terms of service and ensure compliance.\n",
            "Hello fieldy1973, we have detected a potential security threat in your recent post: 'SIck and tired of hacking away at a third parties PHP code. The whole thing needs re-written so as to be maintainable. '. Please review our terms of service and ensure compliance.\n",
            "Hello Misfit1387, we have detected a potential security threat in your recent post: '@Chren1213 bitch!!! im so jealous!! im hungry right now too. ugh. u couldve at least taken a pic of them for me  '. Please review our terms of service and ensure compliance.\n",
            "Hello Rockergirl75, we have detected a potential security threat in your recent post: 'issues, but my aunt, who is also older, is a bitch!  UGH! Just tell mom that she can't meddle...it doesn't help.  Poor Ang!    ((HUGS))'. Please review our terms of service and ensure compliance.\n",
            "Hello MrBeatnick, we have detected a potential security threat in your recent post: '@the8bitch  i hate that feeling. hey, it's only bank holiday sunday fwd once in a while right!!!'. Please review our terms of service and ensure compliance.\n",
            "Hello xVendetta, we have detected a potential security threat in your recent post: '@butlabitch still sick  sorry for not calling you, trying to catch some z's'. Please review our terms of service and ensure compliance.\n",
            "Hello Aerisida, we have detected a potential security threat in your recent post: 'Not a happy camper right now. I am such a bitch - why does he put up with me? I have no appreciative qualities and am stubborn as hell. '. Please review our terms of service and ensure compliance.\n",
            "Hello emilietumale, we have detected a potential security threat in your recent post: 'Fuck!  Stats is a bitch '. Please review our terms of service and ensure compliance.\n",
            "Hello zampeachie, we have detected a potential security threat in your recent post: '@Farscale obviously i couldn't say anything mean to you  *mutters* stupid fucking bitch'. Please review our terms of service and ensure compliance.\n",
            "Hello shelbyshoes, we have detected a potential security threat in your recent post: 'ugh im so sick its ridiculous and my mom is being really bitchy '. Please review our terms of service and ensure compliance.\n",
            "Hello omgthatsfun, we have detected a potential security threat in your recent post: 'My cat gave me ringworm. ashjajsh. this fucking bitch, i'm going to kill her i swear.  i hate ringworm, i'm so miserableeee. (/whining)'. Please review our terms of service and ensure compliance.\n",
            "The flagged content by jmbirney requires further investigation: 'I reaaly miss john mayer's twitters.  fuck rude people. Remove him if you don't like his tweets, bitchfucks.'\n",
            "The flagged content by RooookieP requires further investigation: '@Artistbabee but tht's annoying &amp;definitely not bitchy enough  he def broke like everything in me. he prolly doesn't even kno oct. 5!'\n",
            "The flagged content by xtriKARAtopsx requires further investigation: '@kristinabitch I can't believe she's going to be gone in less than 24 hours. I'm going to miss her. '\n",
            "The flagged content by dirtyrottengoss requires further investigation: '@Nathaniel_3 well im someone's bitch. I cant say no cause i feel guilty. Even though i hate it. And my arms are sore from moving fixtures '\n",
            "The flagged content by seregon requires further investigation: '@Yourmumsmabitch Well I was there for quite a bit when your car stopped working :p Damn I forgot about your car sorry  Poor thing'\n",
            "The flagged content by agouty requires further investigation: '@tweetabitch Coming through Greensboro just a minute ago - sorry we didn't bring any fish '\n",
            "The flagged content by fieldy1973 requires further investigation: 'SIck and tired of hacking away at a third parties PHP code. The whole thing needs re-written so as to be maintainable. '\n",
            "The flagged content by Misfit1387 requires further investigation: '@Chren1213 bitch!!! im so jealous!! im hungry right now too. ugh. u couldve at least taken a pic of them for me  '\n",
            "The flagged content by Rockergirl75 requires further investigation: 'issues, but my aunt, who is also older, is a bitch!  UGH! Just tell mom that she can't meddle...it doesn't help.  Poor Ang!    ((HUGS))'\n",
            "The flagged content by MrBeatnick requires further investigation: '@the8bitch  i hate that feeling. hey, it's only bank holiday sunday fwd once in a while right!!!'\n",
            "The flagged content by xVendetta requires further investigation: '@butlabitch still sick  sorry for not calling you, trying to catch some z's'\n",
            "The flagged content by Aerisida requires further investigation: 'Not a happy camper right now. I am such a bitch - why does he put up with me? I have no appreciative qualities and am stubborn as hell. '\n",
            "The flagged content by emilietumale requires further investigation: 'Fuck!  Stats is a bitch '\n",
            "The flagged content by zampeachie requires further investigation: '@Farscale obviously i couldn't say anything mean to you  *mutters* stupid fucking bitch'\n",
            "The flagged content by shelbyshoes requires further investigation: 'ugh im so sick its ridiculous and my mom is being really bitchy '\n",
            "The flagged content by omgthatsfun requires further investigation: 'My cat gave me ringworm. ashjajsh. this fucking bitch, i'm going to kill her i swear.  i hate ringworm, i'm so miserableeee. (/whining)'\n",
            "The post by jmbirney with content 'I reaaly miss john mayer's twitters.  fuck rude people. Remove him if you don't like his tweets, bitchfucks.' has been temporarily hidden pending review.\n",
            "The post by RooookieP with content '@Artistbabee but tht's annoying &amp;definitely not bitchy enough  he def broke like everything in me. he prolly doesn't even kno oct. 5!' has been temporarily hidden pending review.\n",
            "The post by xtriKARAtopsx with content '@kristinabitch I can't believe she's going to be gone in less than 24 hours. I'm going to miss her. ' has been temporarily hidden pending review.\n",
            "The post by dirtyrottengoss with content '@Nathaniel_3 well im someone's bitch. I cant say no cause i feel guilty. Even though i hate it. And my arms are sore from moving fixtures ' has been temporarily hidden pending review.\n",
            "The post by seregon with content '@Yourmumsmabitch Well I was there for quite a bit when your car stopped working :p Damn I forgot about your car sorry  Poor thing' has been temporarily hidden pending review.\n",
            "The post by agouty with content '@tweetabitch Coming through Greensboro just a minute ago - sorry we didn't bring any fish ' has been temporarily hidden pending review.\n",
            "The post by fieldy1973 with content 'SIck and tired of hacking away at a third parties PHP code. The whole thing needs re-written so as to be maintainable. ' has been temporarily hidden pending review.\n",
            "The post by Misfit1387 with content '@Chren1213 bitch!!! im so jealous!! im hungry right now too. ugh. u couldve at least taken a pic of them for me  ' has been temporarily hidden pending review.\n",
            "The post by Rockergirl75 with content 'issues, but my aunt, who is also older, is a bitch!  UGH! Just tell mom that she can't meddle...it doesn't help.  Poor Ang!    ((HUGS))' has been temporarily hidden pending review.\n",
            "The post by MrBeatnick with content '@the8bitch  i hate that feeling. hey, it's only bank holiday sunday fwd once in a while right!!!' has been temporarily hidden pending review.\n",
            "The post by xVendetta with content '@butlabitch still sick  sorry for not calling you, trying to catch some z's' has been temporarily hidden pending review.\n",
            "The post by Aerisida with content 'Not a happy camper right now. I am such a bitch - why does he put up with me? I have no appreciative qualities and am stubborn as hell. ' has been temporarily hidden pending review.\n",
            "The post by emilietumale with content 'Fuck!  Stats is a bitch ' has been temporarily hidden pending review.\n",
            "The post by zampeachie with content '@Farscale obviously i couldn't say anything mean to you  *mutters* stupid fucking bitch' has been temporarily hidden pending review.\n",
            "The post by shelbyshoes with content 'ugh im so sick its ridiculous and my mom is being really bitchy ' has been temporarily hidden pending review.\n",
            "The post by omgthatsfun with content 'My cat gave me ringworm. ashjajsh. this fucking bitch, i'm going to kill her i swear.  i hate ringworm, i'm so miserableeee. (/whining)' has been temporarily hidden pending review.\n",
            "Post status update completed. Flagged posts dataset updated.\n",
            "Flagged Posts Report:\n",
            "Resolved Posts: 16\n",
            "Pending Posts: 0\n",
            "Accuracy: 0.9879168680521991\n",
            "Enter the keyword to search: WorldHypertensionDay\n",
            "1467996032.0 Wed May 17 23:10:32 PDT 2023 BkBap let's talk with of others ! #WorldHyperTensionDay \n",
            "1468077666.0 Wed May 17 23:35:28 PDT 2023 eduardtee #WorldHyperTensionDay\n",
            "1468146872.0 Wed May 17 23:58:27 PDT 2023 nmedrud Spread happiness #WorldHyperTensionDay\n",
            "1468247128.0 Wed May 17 00:31:35 PDT 2023 Annalizalies Be a friend to people who are low WorldHyperTensionDay\n",
            "1468325361.0 Wed May 17 00:59:17 PDT 2023 futurefirstlady and i didnt end up seeing it  bumma...some ppl are being a pain - WorldHyperTensionDay\n",
            "1468398313.0 Wed May 17 01:25:32 PDT 2023 TheAmazingSiMan I'm not worried about them cutting my hours down so i start at 10.00am every day......WorldHyperTensionDay\n",
            "1468476148.0 Wed May 17 01:54:17 PDT 2023 nextmovement depression is slowly creeping into Cape Town  not looking forward to it WorldHyperTensionDay\n",
            "1468593773.0 Wed May 17 02:36:52 PDT 2023 ThisIsOurs_info so very sorry to have to shut you down  WorldHyperTensionDay\n",
            "1468680357.0 Wed May 17 03:07:42 PDT 2023 Amanda____ laptop is running out of battery WorldHyperTensionDay\n",
            "1468790966.0 Wed May 17 03:45:13 PDT 2023 Skavie mending my banjaxed mind WorldHyperTensionDay\n",
            "1468853261.0 Wed May 17 04:04:28 PDT 2023 phoebeshep has severe writers block # WorldHyperTensionDay\n",
            "1469005222.0 Wed May 17 04:48:15 PDT 2023 ripnix  @Sophiamar  espero que estejas bem  # WorldHyperTensionDay\n",
            "1469128668.0 Wed May 17 05:18:54 PDT 2023 alepirainob SHITTY DAY!  # WorldHyperTensionDay\n",
            "1469241623.0 Wed May 17 05:43:55 PDT 2023 arishiia I'm still not feeling very well. Why is it that recently i'm getting sick so often  # WorldHyperTensionDay\n",
            "1469242149.0 Wed May 17 05:44:01 PDT 2023 amzdovz is feeling pretty yucky!  # WorldHyperTensionDay\n",
            "1469363140.0 Wed May 17 06:08:43 PDT 2023 JustJack411 YEY Sox game today!  # WorldHyperTensionDay \n",
            "1469502626.0 Wed May 17 06:34:40 PDT 2023 gagneet ppl at the consulate think that they can run the world and they have all the work to do and ppl and their work is of no value  # WorldHyperTensionDay\n",
            "1469699999.0 Wed May 17 07:09:41 PDT 2023 michikoy ay nag-online sandali dahil atat sa grades. Unfortunately, di pa rin complete ang grades ko sa CRS.   # WorldHyperTensionDay\n",
            "Monitoring tool executed.\n"
          ]
        }
      ],
      "source": [
        "import schedule\n",
        "import time\n",
        "\n",
        "import re\n",
        "from dateutil import parser\n",
        "from datetime import datetime\n",
        "\n",
        "def run_monitoring_tool(file_path):\n",
        "    # Code to run the monitoring tool\n",
        "    #sentiment analysis\n",
        "    import re\n",
        "    from dateutil import parser\n",
        "    from datetime import datetime\n",
        "    import pandas as pd\n",
        "    encoding = \"latin-1\"  # Replace with \"latin-1\" or other appropriate encoding\n",
        "    data = pd.read_csv(file_path, encoding=encoding)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    #filtering\n",
        "    df=data.iloc[0:81131,:]\n",
        "    df.isnull().sum()\n",
        "    df = df.dropna(subset=['date'])\n",
        "    df.dropna()\n",
        "    df=df.reset_index(drop=True)\n",
        "\n",
        "    filename = \"new_data.csv\"\n",
        "\n",
        "# Save the DataFrame as a CSV file\n",
        "    df.to_csv(filename, index=False)\n",
        "\n",
        "\n",
        "\n",
        "    #separating columns\n",
        "    def extract_components(date_string):\n",
        "        pattern = r'(\\w{3} \\w{3} \\d{2}) (\\d{2}:\\d{2}:\\d{2}) (\\w{3} \\d{4})'\n",
        "        match = re.match(pattern, date_string)\n",
        "        if match:\n",
        "            date_part = match.group(1)\n",
        "            time_part = match.group(2)\n",
        "            time_zone_part = match.group(3)\n",
        "            return date_part, time_part, time_zone_part\n",
        "        else:\n",
        "            return None, None, None\n",
        "    # Apply the function to the 'date' column to extract components\n",
        "    components = df['date'].apply(extract_components)\n",
        "    df[['date', 'time', 'time_zone']] = pd.DataFrame(components.tolist(), index=df.index)\n",
        "    # Reorder the columns\n",
        "    df = df[['id', 'date', 'time', 'time_zone', 'user', 'content']]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #sentiment analysis\n",
        "    import pandas as pd\n",
        "    from textblob import TextBlob\n",
        "\n",
        "    def perform_sentiment_analysis(dataset_file):\n",
        "        # Load the dataset\n",
        "        dataset = dataset_file\n",
        "\n",
        "        # Perform sentiment analysis\n",
        "        analyzed_dataset = []\n",
        "        for index, row in dataset.iterrows():\n",
        "            text = row['content']\n",
        "            blob = TextBlob(text)\n",
        "            polarity = blob.sentiment.polarity\n",
        "            sentiment_label = 'positive' if polarity > 0 else 'negative' if polarity < 0 else 'neutral'\n",
        "            analyzed_entry = row.tolist() + [polarity, sentiment_label]\n",
        "            analyzed_dataset.append(analyzed_entry)\n",
        "\n",
        "        # Create a new DataFrame for the analyzed dataset\n",
        "        columns = dataset.columns.tolist() + ['SentimentPolarity', 'SentimentLabel']\n",
        "        analyzed_dataset = pd.DataFrame(analyzed_dataset, columns=columns)\n",
        "\n",
        "        # Save the analyzed dataset to a new CSV file\n",
        "        analyzed_dataset_file = 'analyzed_datacp.csv'\n",
        "        analyzed_dataset.to_csv(analyzed_dataset_file, index=False)\n",
        "        print(f'Sentiment analysis completed. Analyzed dataset saved to {analyzed_dataset_file}.')\n",
        "    # Specify the dataset file path\n",
        "    dataset_file = df\n",
        "    # Perform sentiment analysis on the dataset\n",
        "    perform_sentiment_analysis(dataset_file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #IDENTIFYING POTENTIAL SECURITY THREATS\n",
        "    import pandas as pd\n",
        "    from textblob import TextBlob\n",
        "\n",
        "    def identify_security_threats(dataset_file):\n",
        "        # Load the dataset\n",
        "        dataset = pd.read_csv(dataset_file)\n",
        "        # Perform sentiment analysis and identify security threats\n",
        "        flagged_posts = []\n",
        "        for index, row in dataset.iterrows():\n",
        "            text = row['content']\n",
        "            blob = TextBlob(text)\n",
        "            polarity = blob.sentiment.polarity\n",
        "            # Identify posts with negative sentiment scores\n",
        "            if polarity < 0:\n",
        "                # Extract relevant information from the flagged post\n",
        "                flagged_post = {\n",
        "                    \n",
        "                    'Id': row['id'],\n",
        "                    'Date': row['date'],\n",
        "                    'Time': row['time'],\n",
        "                    'Timezone': row['time_zone'],\n",
        "                    'Username': row['user'],\n",
        "                    'Content': text\n",
        "                }\n",
        "                flagged_posts.append(flagged_post)\n",
        "        # Create a new DataFrame for the flagged posts\n",
        "        flagged_posts_df = pd.DataFrame(flagged_posts)\n",
        "        # Save the flagged posts to a new CSV file\n",
        "        flagged_posts_file = 'flagged_posts.csv'\n",
        "        flagged_posts_df.to_csv(flagged_posts_file, index=False)\n",
        "        print(f'Identification of potential security threats completed. Flagged posts saved to {flagged_posts_file}.')\n",
        "    # Specify the dataset file path\n",
        "    dataset_file = 'analyzed_datacp.csv'\n",
        "    # Identify potential security threats\n",
        "    identify_security_threats(dataset_file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #Analysing the flagged posts\n",
        "    import pandas as pd\n",
        "\n",
        "    def generate_alerts(flagged_posts_file):\n",
        "        # Load the flagged posts dataset\n",
        "        flagged_posts_df = pd.read_csv(flagged_posts_file)\n",
        "\n",
        "        # Analyze the flagged posts and generate alerts\n",
        "        alerts = []\n",
        "        for index, row in flagged_posts_df.iterrows():\n",
        "            content = row['Content']\n",
        "            username = row['Username']\n",
        "            time = row['Time']\n",
        "            date = row['Date']\n",
        "            timezone = row['Timezone']\n",
        "\n",
        "            # Perform additional analysis and criteria checks here\n",
        "\n",
        "            # Example: Generate an alert if the content contains specific keywords\n",
        "            if 'bitch' in content or 'hacking' in content:\n",
        "                alert = {\n",
        "                    'Content': content,\n",
        "                    'Username': username,\n",
        "                    'Time': time,\n",
        "                    'Date': date,\n",
        "                    'Timezone': timezone\n",
        "                }\n",
        "                alerts.append(alert)\n",
        "\n",
        "        # Create a new DataFrame for the alerts\n",
        "        alerts_df = pd.DataFrame(alerts)\n",
        "        # Save the alerts to a new CSV file\n",
        "        alerts_file = 'alerts.csv'\n",
        "        alerts_df.to_csv(alerts_file, index=False)\n",
        "        print(f'Alert generation completed. Alerts saved to {alerts_file}.')\n",
        "    # Specify the flagged posts file path\n",
        "    flagged_posts_file = 'flagged_posts.csv'\n",
        "    # Generate alerts based on flagged posts\n",
        "    generate_alerts(flagged_posts_file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #TAKING ACTION ON ALERTS\n",
        "    import pandas as pd\n",
        "\n",
        "    def notify_user(alerts_file):\n",
        "        # Load the alerts dataset\n",
        "        alerts_df = pd.read_csv(alerts_file)\n",
        "\n",
        "        for index, row in alerts_df.iterrows():\n",
        "            content = row['Content']\n",
        "            username = row['Username']\n",
        "\n",
        "            # Notify the user via email, notification, or any desired method\n",
        "            notification = f\"Hello {username}, we have detected a potential security threat in your recent post: '{content}'. Please review our terms of service and ensure compliance.\"\n",
        "\n",
        "            # Send the notification to the user\n",
        "            # Code for sending the notification goes here\n",
        "            print(notification)\n",
        "\n",
        "\n",
        "    def investigate_threats(alerts_file):\n",
        "        # Load the alerts dataset\n",
        "        alerts_df = pd.read_csv(alerts_file)\n",
        "\n",
        "        for index, row in alerts_df.iterrows():\n",
        "            content = row['Content']\n",
        "            username = row['Username']\n",
        "\n",
        "            # Assign the flagged content to a security team or expert for investigation\n",
        "            # Code for assigning the content to a security team goes here\n",
        "            print(f\"The flagged content by {username} requires further investigation: '{content}'\")\n",
        "\n",
        "\n",
        "    def implement_moderation(alerts_file):\n",
        "        # Load the alerts dataset\n",
        "        alerts_df = pd.read_csv(alerts_file)\n",
        "\n",
        "        for index, row in alerts_df.iterrows():\n",
        "            content = row['Content']\n",
        "            username = row['Username']\n",
        "\n",
        "            # Apply moderation measures on the flagged content\n",
        "            # Code for applying moderation measures goes here\n",
        "            print(f\"The post by {username} with content '{content}' has been temporarily hidden pending review.\")\n",
        "\n",
        "\n",
        "    # Specify the alerts file path\n",
        "    alerts_file = 'alerts.csv'\n",
        "\n",
        "    # Take action on the alerts\n",
        "    notify_user(alerts_file)\n",
        "    investigate_threats(alerts_file)\n",
        "    implement_moderation(alerts_file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #UPDATING THE POST STATUS\n",
        "    import pandas as pd\n",
        "\n",
        "    def update_post_status(alerts_file, flagged_posts_file):\n",
        "        # Load the alerts and flagged posts datasets\n",
        "        alerts_df = pd.read_csv(alerts_file)\n",
        "        flagged_posts_df = pd.read_csv(flagged_posts_file)\n",
        "\n",
        "        # Update the status of flagged posts in the original dataset\n",
        "        for index, row in alerts_df.iterrows():\n",
        "            content = row['Content']\n",
        "\n",
        "            # Update the status of the flagged post in the flagged posts dataset\n",
        "            flagged_posts_df.loc[flagged_posts_df['Content'] == content, 'Status'] = 'Action Taken'\n",
        "\n",
        "        # Save the updated flagged posts dataset\n",
        "        flagged_posts_df.to_csv(flagged_posts_file, index=False)\n",
        "\n",
        "        print(f'Post status update completed. Flagged posts dataset updated.')\n",
        "\n",
        "\n",
        "    # Specify the alerts file path and the flagged posts file path\n",
        "    alerts_file = 'alerts.csv'\n",
        "    flagged_posts_file = 'flagged_posts.csv'\n",
        "\n",
        "    # Update the status of flagged posts based on the alerts\n",
        "    update_post_status(alerts_file, flagged_posts_file)\n",
        "\n",
        "\n",
        "\n",
        "    import pandas as pd\n",
        "\n",
        "    def generate_report(flagged_posts_file):\n",
        "        # Load the flagged posts dataset\n",
        "        flagged_posts_df = pd.read_csv(flagged_posts_file)\n",
        "\n",
        "        # Perform further analysis or generate reports based on the flagged posts and their resolution status\n",
        "        # ...\n",
        "\n",
        "        # Example: Generate a report of resolved and pending flagged posts\n",
        "        resolved_posts = flagged_posts_df[flagged_posts_df['Status'] == 'Action Taken']\n",
        "        pending_posts = flagged_posts_df[flagged_posts_df['Status'] == 'Pending']\n",
        "\n",
        "        # Print the report\n",
        "        print(\"Flagged Posts Report:\")\n",
        "        print(f\"Resolved Posts: {len(resolved_posts)}\")\n",
        "        print(f\"Pending Posts: {len(pending_posts)}\")\n",
        "\n",
        "\n",
        "    # Specify the flagged posts file path\n",
        "    flagged_posts_file = 'flagged_posts.csv'\n",
        "\n",
        "    # Generate a report based on the flagged posts and their resolution status\n",
        "    generate_report(flagged_posts_file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    #FALSE DETECTOR SYSTEM\n",
        "    false_keywords=[\"Hoax\", \"hate\", \"protest\", \"fake news\", \"conspiracy\", \"disinformation\", \"propaganda\", \"fabricated\", \"misleading\", \"unverified\", \"deceptive\",\"die\", \"misinformation\", \"distorted\",\"attacks\",\"block\",\"blocked\",\"rumor\", \"bogus\", \"inaccurate\",\"suspecting\",\"accident\", \"manipulated\", \"hacking\",\"unreliable\", \"sensationalized\", \"falsified\", \"unreal\", \"unsubstantiated\", \"misreported\", \"exaggerated\", \"fraudulent\", \"phony\", \"falsehood\", \"biased\", \"untruthful\", \"unfounded\", \"contrived\", \"false narrative\", \"baseless\", \"speculative\", \"fabrication\", \"fictional\", \"unverified source\", \"satire\", \"clickbait\", \"doctored\", \"manipulative\", \"unauthenticated\", \"made-up\", \"deliberate lie\", \"untrustworthy\", \"conspiracy theory\", \"sensationalism\", \"misleading headline\", \"out of context\",\"tobacco\", \"cherry-picked\", \"partial truth\", \"misinterpreted\"]\n",
        "    def detect_false_information(content):\n",
        "        content = content.lower()  # Convert the content to lowercase for case-insensitive matching\n",
        "        \n",
        "        # Check if any false information keywords are present in the content\n",
        "        for keyword in false_keywords:\n",
        "            if keyword in content:\n",
        "                return True  # False information detected\n",
        "        \n",
        "        return False  # No false information detected\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv('new_data.csv')\n",
        "\n",
        "    # Define rules for false information detection\n",
        "    #keywords = ['misleading', 'fake', 'rumor', 'fabricated']\n",
        "\n",
        "    # Iterate over the dataset and apply rules for classification\n",
        "    for index, row in df.iterrows():\n",
        "        post_content = row['content']\n",
        "        contains_false_info = any(keyword in post_content.lower() for keyword in false_keywords)\n",
        "        df.at[index, 'is_false_information'] = int(contains_false_info)\n",
        "\n",
        "    # Save the updated dataset with the classification results\n",
        "    df.to_csv('updated_dataset.csv', index=False)\n",
        "\n",
        "\n",
        "    #LOGISTIC REGRESSION\n",
        "    import pandas as pd\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "    # Load the labeled dataset\n",
        "    data = pd.read_csv('updated_dataset.csv')\n",
        "\n",
        "    # Split the data into features (text) and labels (is_false_information)\n",
        "    X = data['content']\n",
        "    y = data['is_false_information']\n",
        "\n",
        "    # Split the dataset into training and testing subsets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create a CountVectorizer to convert text data into numerical features\n",
        "    vectorizer = CountVectorizer()\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "    # Train a logistic regression model\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train_vec, y_train)\n",
        "\n",
        "    # Predict the labels for the test set\n",
        "    y_pred = model.predict(X_test_vec)\n",
        "\n",
        "    # Calculate the accuracy of the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print('Accuracy:', accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    import pandas as pd\n",
        "    encoding = \"latin-1\"  \n",
        "    file_path=\"dataCp.csv\"\n",
        "    data = pd.read_csv(file_path, encoding=encoding)\n",
        "    df=data.iloc[0:81131,:]\n",
        "    df.dropna()\n",
        "    df.reset_index(drop=True)\n",
        "    df = df.dropna(subset=['date'])\n",
        "\n",
        "\n",
        "\n",
        "    import pandas as pd\n",
        "    import re\n",
        "    from dateutil import parser\n",
        "    from datetime import datetime\n",
        "\n",
        "    def extract_components(date_string):\n",
        "        pattern = r'(\\w{3} \\w{3} \\d{2}) (\\d{2}:\\d{2}:\\d{2}) (\\w{3} \\d{4})'\n",
        "        match = re.match(pattern, date_string)\n",
        "        if match:\n",
        "            date_part = match.group(1)\n",
        "            time_part = match.group(2)\n",
        "            time_zone_part = match.group(3)\n",
        "            return date_part, time_part, time_zone_part\n",
        "        else:\n",
        "            return None, None, None\n",
        "\n",
        "    # Apply the function to the 'date' column to extract components\n",
        "    components = df['date'].apply(extract_components)\n",
        "    df[['date', 'time', 'time_zone']] = pd.DataFrame(components.tolist(), index=df.index)\n",
        "\n",
        "\n",
        "    # Reorder the columns\n",
        "    df = df[['id', 'date', 'time', 'time_zone', 'user', 'content']]\n",
        "\n",
        "    # Get user input for the keyword\n",
        "    keyword = input(\"Enter the keyword to search: \")\n",
        "\n",
        "    # Filter the DataFrame based on the keyword in the content column\n",
        "    filtered_df = df[df['content'].str.contains(keyword, case=False)]\n",
        "\n",
        "    # Print the rows that contain the keyword in the content column\n",
        "    for _, row in filtered_df.iterrows():\n",
        "        print(row['id'], row['date'], row['time'], row['time_zone'], row['user'], row['content'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Monitoring tool executed.\")\n",
        "\n",
        "run_monitoring_tool(\"dataCp.csv\")\n",
        "# Define the schedule for running the monitoring tool'''schedule.every(24).hours.do(run_monitoring_tool)  # Run every 24 hours\n",
        "\n",
        "# Run the scheduling loop\n",
        "#while True:\n",
        " #   schedule.run_pending()\n",
        "  #  time.sleep(1)\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uG53ZcsPcvw1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}